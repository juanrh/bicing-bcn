package org.collprod.bicingbcn.ingestion;

import java.util.Map;
import java.util.Properties;
import java.util.concurrent.TimeUnit;

import kafka.javaapi.producer.Producer;
import kafka.producer.KeyedMessage;
import kafka.producer.ProducerConfig;

import org.collprod.bicingbcn.ingestion.attic.AvroWriterBolt;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import backtype.storm.task.OutputCollector;
import backtype.storm.task.TopologyContext;
import backtype.storm.topology.OutputFieldsDeclarer;
import backtype.storm.topology.base.BaseRichBolt;
import backtype.storm.tuple.Tuple;

import com.google.auto.value.AutoValue;
import com.google.common.cache.CacheBuilder;
import com.google.common.cache.CacheLoader;
import com.google.common.cache.LoadingCache;

/**
 * Accepts tuples (DATASOURCE_ID, TIMESTAMP_FIELD, CONTENT_FIELD) which are sent to Kafka:
 * - using the  DATASOURCE_ID as Kafka topic
 * - using DATASOURCE_ID + TIMESTAMP_FIELD as Kafka partition key
 * - the message is the CONTENT_FIELD
 * 
 * Check with 
 [cloudera@localhost kafka_2.10-0.8.1.1]$ bin/kafka-console-consumer.sh --zookeeper localhost:2181 --topic test_bicing_station_data
 * */

public class KafkaWriterBolt extends BaseRichBolt {

	// generated by Eclipse
	private static final long serialVersionUID = 3874964577899393556L;
	private static final long TIMESTAMP_SET_MAX_SIZE = 40;
	private static final long TIMESTAMP_SET_EXPIRE_TIME = 3;
	
	private static final Logger LOGGER = LoggerFactory.getLogger(AvroWriterBolt.class);
	
	/**
	 * Storm collector for acking or failing
	 * */
	private OutputCollector collector;
	
	/**
	 * Connection to Kafka as a producer. Uses String for the partition key, and String in the message
	 * */
	private Producer<String, String> kafkaProducer;
	
	/**
	 * Cache representing the set of timestamps seen lately
	 * */
	private LoadingCache<DatasourceTimestamp, Boolean> timestampSeen;
	
	@AutoValue
	static abstract class DatasourceTimestamp {
		DatasourceTimestamp() {}
		public static DatasourceTimestamp create(String datasource, Long timestamp) {
			return new AutoValue_KafkaWriterBolt_DatasourceTimestamp(datasource, timestamp);
		}
		public abstract String datasource();
		public abstract Long timestamp();
	}
	
	/**
	 * To represent the set of the last seen timestamps, we use a cache from Long to Boolean (to use something),
	 * so being a key in the cache is equivalent to appearing in the set  
	 * */
	private LoadingCache<DatasourceTimestamp, Boolean> buildSeenTimestampsSetCache() {
		CacheLoader<DatasourceTimestamp, Boolean> loader = 
				new CacheLoader<DatasourceTimestamp, Boolean> () {
					@Override
					public Boolean load(DatasourceTimestamp key) {
						// Here we treat Boolean as a Unit type. The point is that key
						// now is in the key set, we only use the cache for the eviction
						// capabilities to control the cache size
						return true;
					}
		};
		
		return CacheBuilder.newBuilder()
				.maximumSize(TIMESTAMP_SET_MAX_SIZE)
				.expireAfterAccess(TIMESTAMP_SET_EXPIRE_TIME, TimeUnit.MINUTES)
				.build(loader);
	}
	
	@Override
	public void prepare(Map stormConf, TopologyContext context, OutputCollector collector) {
		this.collector = collector;
		
		Properties kafkaProperties = new Properties();
		int kafkaPrefixLenght = "kafka.".length();
		for (Map.Entry<String, String> stormConfEntry : ((Map<String, String>) stormConf).entrySet()) {
			String key = stormConfEntry.getKey(); 
			if (key.startsWith("kafka.")) {
				kafkaProperties.put(key.substring(kafkaPrefixLenght), stormConfEntry.getValue());
			}
		}
		this.kafkaProducer = new Producer<String, String>(new ProducerConfig(kafkaProperties));
		this.timestampSeen = buildSeenTimestampsSetCache();
	}
	
	@Override
	public void execute(Tuple inputTuple) {
		/* Processing tuples of the shape
		   (DATASOURCE_ID, TIMESTAMP_FIELD, CONTENT_FIELD) */
		// get data
		String datasource = inputTuple.getStringByField(RestIngestionSpout.DATASOURCE_ID);
		Long timestamp = inputTuple.getLongByField(RestIngestionSpout.TIMESTAMP_FIELD);
		
		DatasourceTimestamp  datasourceTimestamp = DatasourceTimestamp.create(datasource, timestamp);
		Boolean tupleAlreadySeen = this.timestampSeen.getIfPresent(datasourceTimestamp);
		if (tupleAlreadySeen == null) {
			// we haven't seen this timestamp for this datasource, add to the cache / set
			this.timestampSeen.getUnchecked(datasourceTimestamp);
		} else {
			// we have already seen this timestamp for this datasource
			// skip. This is relevant for replayed tuples if any (if AvroWriterBolt was working,
			// for example)
			LOGGER.debug("Repeated tuple {}", datasourceTimestamp);
			return;
		}
		
		String content = inputTuple.getStringByField(RestIngestionSpout.CONTENT_FIELD);
				
		try {
			this.kafkaProducer.send(new KeyedMessage<String, String>
				// Kafka topic 
				(datasource, 
				// Partition key
				datasource + timestamp.toString(),
				// Message
				content));
		} catch (Exception e) {
			// Exceptions in Kafka API are not checked
			LOGGER.error("Error sending message to Kafka, will retry {}", e.getMessage());
			this.collector.fail(inputTuple);
			return;
		}
		
		// ack this tuple
		this.collector.ack(inputTuple);
	}

	/**
	 * This is useless in cluster mode as Storm doesn't call this method,  
	 * so I got that going on for me, which is nice 
	 * */
	@Override
	public void cleanup() {
		this.kafkaProducer.close();
	}

	@Override
	public void declareOutputFields(OutputFieldsDeclarer declarer) {
		// this bolts emits no tuples, just pushes them to Kafka
	}

}
