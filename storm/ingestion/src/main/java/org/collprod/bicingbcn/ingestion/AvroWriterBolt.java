package org.collprod.bicingbcn.ingestion;

import java.io.IOException;
import java.text.SimpleDateFormat;
import java.util.Date;
import java.util.HashMap;
import java.util.Map;

import org.apache.avro.Schema;
import org.apache.avro.SchemaBuilder;
import org.apache.avro.file.DataFileWriter;
import org.apache.avro.generic.GenericRecord;
import org.apache.commons.configuration.Configuration;
import org.apache.commons.configuration.ConfigurationException;
import org.apache.hadoop.fs.FileSystem;
import org.collprod.bicingbcn.ingestion.commons.MutableOptionalObject;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import backtype.storm.task.OutputCollector;
import backtype.storm.task.TopologyContext;
import backtype.storm.topology.OutputFieldsDeclarer;
import backtype.storm.topology.base.BaseRichBolt;
import backtype.storm.tuple.Tuple;

import com.google.auto.value.AutoValue;
/**
 * Writes the data to the HDFS path specified in the configuration for this data source
 * Data is serialized in Avro with an Avro file per month, hence it is stored in the path
 * <data source HDFS path>/<month>.avro as a new Avro record with the name AvroWriterBolt.AVRO_RECORD_NAME, 
 * and with the fields:
 * - AVRO_TIMESTAMP_FIELD :: long
 * - AVRO_CONTENT_FIELD :: string
 * 
 * No additional timestamp check is performed, this bolt asssumes that all the data is new
 * */
public class AvroWriterBolt extends BaseRichBolt {
	
	// Generated by Eclipse 
	private static final long serialVersionUID = 117628243688887424L;
	
	private static final Logger LOGGER = LoggerFactory.getLogger(AvroWriterBolt.class);
	private static final SimpleDateFormat MONTH_FORMATTER = new SimpleDateFormat("yyyy-MM-dd");
	
	public static final String AVRO_RECORD_NAME = "data";
	public static final String AVRO_RECORD_NAMESPACE = "org.collprod";
	public static final String AVRO_TIMESTAMP_FIELD = "timestamp";
	public static final String AVRO_CONTENT_FIELD = "content";
	public static final Schema AVRO_SCHEMA;
	static {
		AVRO_SCHEMA = SchemaBuilder
				.record(AVRO_RECORD_NAME).namespace(AVRO_RECORD_NAMESPACE)
				.fields()
					.name(AVRO_TIMESTAMP_FIELD).type(Schema.create(Schema.Type.LONG)).noDefault()
					.name(AVRO_CONTENT_FIELD).type(Schema.create(Schema.Type.STRING)).noDefault()
				.endRecord();
	}
	
	private FileSystem hdfs;	
	
	/**
	 * State for each data source
	 * */
	private Map<String, DatasourceState> datasourceStates;
	
	@AutoValue
	static abstract class DatasourceState {
		DatasourceState() {}
		public static DatasourceState create(String targetDirectory, 
											 MutableOptionalObject<String> currentTargetMonth,
											 MutableOptionalObject<DataFileWriter<GenericRecord>> currentTargetWriter) {
	        return new AutoValue_AvroWriterBolt_DatasourceState(targetDirectory, currentTargetMonth,
	        								 currentTargetWriter);
	      }
		
		/**
		 * Target directory for the datasource
		 * 	
		 * Kind: inmutable configuration
		 * */
		public abstract String targetDirectory();
		/**
		 * Current month in the same format as MONTH_FORMATTER. 
		 * The base name of the target file will be this value with the 
		 * extension .avro added at the end
		 * 
		 * Kind: mutable state
		 * */
		public abstract MutableOptionalObject<String> currentTargetMonth();
		
		/**
		 * Current DataFileWriter
		 * 
		 * Kind: mutable state
		 * */
		public abstract MutableOptionalObject<DataFileWriter<GenericRecord>> currentTargetWriter();
	}
	
	@Override
	public void prepare(@SuppressWarnings("rawtypes") Map stormConf, TopologyContext context,
			OutputCollector collector) {
		// Load target datasource directories from Storm configuration 
		this.datasourceStates = new HashMap<String, DatasourceState>(); 
		try {
			Map<String, Configuration> datasourcesConfigurations = 
					IngestionTopology.deserializeConfigurations((Map<String, String>) stormConf.get(IngestionTopology.DATASOURCE_CONF_KEY));
			for (Map.Entry<String, Configuration> datasourceConfig : datasourcesConfigurations.entrySet()) {
				this.datasourceStates.put(datasourceConfig.getKey(), 
						DatasourceState.create(datasourceConfig.getValue().getString("hdfs_path"),
								// created absent 
								new MutableOptionalObject<String>(),
								// created absent 
								new MutableOptionalObject<DataFileWriter<GenericRecord>>())
						
						);
			}
		} catch (ConfigurationException ce) {
			LOGGER.error("Error parsing datasource configurations: " + ce.getMessage());
			throw new RuntimeException(ce);
		}
		
		// Create objects to interact with HDFS
			// This configuration reads from the default files
		org.apache.hadoop.conf.Configuration conf = new org.apache.hadoop.conf.Configuration(true);
		try {
			this.hdfs = FileSystem.get(conf);
		} catch (IOException ioe) {
			LOGGER.error("Error connecting to HDFS: " + ioe.getMessage());
			throw new RuntimeException(ioe);
		}

	}

	@Override
	public void execute(Tuple inputTuple) {
		// get state for the input data source
		String datasource = inputTuple.getStringByField("RestIngestionSpout.DATASOURCE_ID");
		DatasourceState datasourceState = this.datasourceStates.get(datasource);
		
		// compute month
		long timestamp = inputTuple.getLongByField(TimestampParserBolt.TIMESTAMP_FIELD);
			// this computation is completely stateless 
		String month = MONTH_FORMATTER.format(new Date(timestamp)); 
		
		

		// TODO don't forget to ACK or fail 
	}

	@Override
	public void declareOutputFields(OutputFieldsDeclarer declarer) {
		// this bolts emits no tuples, just stores in HDFS
	}

}
