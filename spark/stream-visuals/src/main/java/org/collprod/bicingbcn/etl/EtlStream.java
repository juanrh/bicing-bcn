package org.collprod.bicingbcn.etl;

import java.io.BufferedReader;
import java.io.InputStreamReader;
import java.sql.Connection;
import java.sql.DriverManager;
import java.sql.Statement;
import java.util.ArrayList;
import java.util.HashMap;
import java.util.Iterator;
import java.util.List;
import java.util.Map;

import org.apache.commons.configuration.ConfigurationException;
import org.apache.commons.configuration.PropertiesConfiguration;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.function.FlatMapFunction;
import org.apache.spark.api.java.function.Function;
import org.apache.spark.api.java.function.PairFunction;
import org.apache.spark.broadcast.Broadcast;
import org.apache.spark.streaming.Duration;
import org.apache.spark.streaming.api.java.JavaDStream;
import org.apache.spark.streaming.api.java.JavaPairDStream;
import org.apache.spark.streaming.api.java.JavaPairReceiverInputDStream;
import org.apache.spark.streaming.api.java.JavaStreamingContext;
import org.apache.spark.streaming.kafka.KafkaUtils;
import org.collprod.bicingbcn.BicingStationDao;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import scala.Tuple2;

import com.atlassian.fugue.Pair;
import com.google.common.collect.Lists;

public class EtlStream {

	private static final Logger LOGGER = LoggerFactory.getLogger(EtlStream.class);

	// FIXME: names
	@SuppressWarnings("unchecked")
	private static final List<Pair<String, String>> CONFIGURATIONS = 
			Lists.newArrayList(new Pair<String, String>("main", "visuals.properties"));
	

	private static PropertiesConfiguration loadConfiguration() throws ConfigurationException {
		LOGGER.info("Loading configuration");
		PropertiesConfiguration config = new PropertiesConfiguration();
		for (Pair<String, String> descriptionPath : CONFIGURATIONS) {
			PropertiesConfiguration newConf = new PropertiesConfiguration();
			newConf.load(new BufferedReader(new InputStreamReader(
					EtlStream.class.getResourceAsStream("/" + descriptionPath.right()))));
			config.append(newConf);
			LOGGER.info("Done loading " + descriptionPath.left() + "configuration");
		}
		LOGGER.info("Done loading configuration");
		
		return config;
	}
	
	public static void main(String[] args) throws ConfigurationException {
		// TODO: change project name in maven to spark-stream and regenerate Eclipse project
		
		// Load program configuration
		PropertiesConfiguration config = loadConfiguration();
		
		// Connect to Spark cluster
		JavaStreamingContext jssc = new JavaStreamingContext(config.getString("spark.master"), 
				config.getString("spark.app_name"), 												
				new Duration(config.getLong("spark.batch_duration")));
		
		// Connect to Kafka
		Map<String,Integer> kafkaTopics = new HashMap<String, Integer>();
		// See https://github.com/apache/spark/blob/master/examples/src/main/java/org/apache/spark/examples/streaming/JavaKafkaWordCount.java
		// that 1 means 1 thread of the Kafka consumer for consuming this topic, TODO put in configuration 
		kafkaTopics.put(config.getString("kafka.bicing_topic"), 2);
		/* - First element is the Kafka partition key (see org.collprod.bicingbcn.ingestion.KafkaWriterBolt to
		 *  check that the partition key is datasource + timestamp.toString(), e.g. test_bicing_station_data1404244632)
		 * - Second element is the message itself
		 */
		JavaPairReceiverInputDStream<String,String> kafkaStream = KafkaUtils.createStream(jssc, 
				config.getString("kafka.zookeeper.quorum"),
				config.getString("kafka.groupid"),
				kafkaTopics);
			
		final Broadcast<BicingStationDao> bicingStationDao = jssc.sparkContext().broadcast(new BicingStationDao());
		
		// Parse XML data into BicingStationDao.Value objects
		JavaDStream<BicingStationDao.Value> bicingStationStates = kafkaStream.flatMap(new FlatMapFunction<Tuple2<String, String>, BicingStationDao.Value>() {
			// generated by Eclipse
			private static final long serialVersionUID = -164175401233776623L;

			/**
			 * - First element is the Kafka partition key (see org.collprod.bicingbcn.ingestion.KafkaWriterBolt to
			 * check that the partition key is datasource + timestamp.toString(), e.g. test_bicing_station_data1404244632)
			 * - Second element is the message itself
			 * */
			@Override
			public Iterable<BicingStationDao.Value> call(Tuple2<String, String> kafkakeyMessage)
					throws Exception {
				// return sharedBicingStationDao.value().parse(kafkakeyMessage._2); // works in local mode
				return bicingStationDao.value().parse(kafkakeyMessage._2);
			}
		});
		
		// FIXME hardcoded, load from config and broadcast
		// JDBC driver name and database URL
		final String JDBC_DRIVER = "org.apache.phoenix.jdbc.PhoenixDriver";
		final String DB_URL = "jdbc:phoenix:localhost";
		
		// Note: trusting in kafka ordering warranties	
		// the state is just an integer with the number of bikes in the station for 
		// the last update
		JavaPairDStream<Integer, BicingStationDao.Value> stationStatePairs = 
				bicingStationStates.mapToPair(new PairFunction<BicingStationDao.Value, Integer, BicingStationDao.Value>() {
					private static final long serialVersionUID = 3493170766978489850L;

					@Override
					public Tuple2<Integer, BicingStationDao.Value> call(BicingStationDao.Value stationInfo) throws Exception {
						return new Tuple2<Integer, BicingStationDao.Value>(stationInfo.id(), stationInfo);
					}
				});
		
		stationStatePairs.print();
		/*
		 * HERE:
		 * - use updateStateByKey to generate a JavaPairDStream using the number of bikes in the last batch 
		 * as state. In case a more complex state is needed use an autovalue
		 * - in foreachRDD generate a pairRDD using the timestamp and use rdd.sortByKey() so station updates
		 * in this batch are treated in order. This together with Kafka order warranties should be enough
		 * 
		 * */
		
//		@AutoValue
//		public static abstract class EtlState implements Serializable {
//			public static EtlState create(int lastNumBikes)
//			
//			EtlState() {}
//			public abstract int lastNumBikes();
//		}
		/*
		 * 

@AutoValue
	public static abstract class Value implements Serializable {
		// generated by Eclipse
		private static final long serialVersionUID = 8581922474241143038L;
		
		Value() {}
		public static Value create(long updatetime, int id, double latitude,
				double longitude, String street, int height, int streetNumber,
				ArrayList<Integer> nearbyStationList, String status, int slots,
				int bikes) {
	        return new AutoValue_BicingStationDao_Value(updatetime, id, latitude,
					longitude, street, height, streetNumber, nearbyStationList, status, 
					slots, bikes);
	      }
		public abstract long updatetime();
		public abstract int id();
		public abstract double latitude();
		public abstract double longitude();
		public abstract String street();
		public abstract int height();
		public abstract int streetNumber();
		// NOTE: it's essential to use ArrayList instead of List, otherwise this class
		// won't be Serializable and thus NotSerializableException when used in Window operations
		public abstract ArrayList<Integer> nearbyStationList();
		public abstract String status();
		public abstract int slots();
		public abstract int bikes();
	}
*/
		
		// This is working writing to Phoenix
		bicingStationStates.foreachRDD(new Function<JavaRDD<BicingStationDao.Value>, Void>() {

			@Override
			public Void call(JavaRDD<BicingStationDao.Value> stationsData) throws Exception {
				stationsData.mapPartitions(new FlatMapFunction<Iterator<BicingStationDao.Value>, Void>() {
					private static final long serialVersionUID = -5770319181680664525L;

					@Override
					public Iterable<Void> call(Iterator<BicingStationDao.Value> stationData) throws Exception {
						LOGGER.info("writing to Phoenix!");
						// Register JDBC driver
						Class.forName(JDBC_DRIVER);
						Connection con = DriverManager.getConnection(DB_URL);
					    // FIXME harcoded phoenix host
//						Connection con = DriverManager.getConnection("jdbc:phoenix:[localhost]");
						Statement stmt = con.createStatement();
						stmt.executeUpdate("UPSERT INTO BICING_DIM_STATION VALUES (26, 2.18198100, 41.4070350, 28, 'Eixample', 'Sagrada Fam√≠lia', '08025', 'Dos Maig 230', 35678.34, 266874, 7.48)");
						con.commit();
						con.close();
						return new ArrayList<Void>();
					}
				}).count();
								
				return null;
			}
		});
		
		
//		bicingStationStates.print();
		
		// Start Spark stream and await for termination
		jssc.start();
		jssc.awaitTermination();
		
	}

}
